{
  "entries": [
    {
      "seminar_title": "Empirical Explorations of Optimization and Generalization for Neural Nets",
      "date": "8-Sept-25",
      "location": "Room 903 SSW, 1255 Amsterdam Avenue",
      "time": "4:10 pm - 5:00 pm",
      "speaker": "Micah Goldblum",
      "affiliation": "Columbia University",
      "abstract": "The fundamental building blocks of machine learning are (1) optimization or fitting the training data and (2) generalization or extrapolating from the training data to unseen samples. In this talk, we will explore both of these topics in the context of neural networks. First, we will inspect neural network loss functions and what they can tell us about generalization. Then, we will discuss recent work on stabilizing optimization for language models. We show that vanilla SGD without momentum is nearly as fast as Adam in the small-batch regime.",
      "bio": "(goldblum.github.io/): Micah Goldblum is an assistant professor in the department of electrical engineering at Columbia University. Before his current position, Micah was a postdoctoral researcher at New York University with Yann LeCun and Andrew Gordon Wilson. Micah’s research focuses on both applied and fundamental problems in machine learning including AI safety, automated data science, training and inference strategies for large-scale models, and building a mathematical and also scientific understanding of why complex AI systems work."
    },
    {
      "seminar_title": "Tailoring matrices to boost spectral algorithms",
      "date": "15-Sept-25",
      "location": "Room 903 SSW, 1255 Amsterdam Avenue",
      "time": "4:10 pm - 5:00 pm",
      "speaker": "Tim Kunisky",
      "affiliation": "Johns Hopkins",
      "abstract": "Spectral algorithms, ones performing hypothesis testing or estimation by looking at the eigenvalues and eigenvectors of a suitable matrix, are one of the central computational techniques of high-dimensional statistics. They enjoy the advantages of being simple to define, efficient, robust, and often straightforward to analyze using the powerful machinery of random matrix theory. However, this talk will show that, in several widely studied settings, the \"naive\" spectral algorithm one might naturally reach for is suboptimal, and can be improved on by choosing a subtler matrix (or matrices) whose spectrum to study.\nI will present two illustrative vignettes. In the planted clique problem over graphs, the natural spectral algorithm applied to the adjacency matrix can be surpassed by instead working with a \"nonlinear Laplacian\", a linear combination of the adjacency matrix and a nonlinear function of the diagonal degree matrix. Similarly, in the synchronization problem over finite or compact groups, various ad hoc spectral methods are inferior to combining spectral information from several matrices chosen according to the representation theory of the underlying group. Through these examples, I will argue that much remains to be understood about spectral algorithms: even in these basic models and for algorithms just a step beyond the simplest ones, numerous subtle phenomena of random matrix theory arise, and optimally tuning these more flexible algorithms already appears to be a task currently amenable only to heuristics and numerical experiments. Finally, time permitting, I will propose a broader research program of using equivariant matrix-to-matrix functions to systematically design matrices for spectral methods and discuss some of the theoretical challenges this direction poses.\nBased on joint work with Yuxin Ma and Shujing Chen.",
      "bio": "Tim Kunisky is an Assistant Professor in the Department of Applied Mathematics and Statistics at Johns Hopkins University."
    },
    {
      "seminar_title": "Veridical Data Science towards Cost-Effective Hypothesis Generation in Science",
      "date": "24-Sept-25",
      "location": "Room 903 SSW, 1255 Amsterdam Avenue",
      "time": "4:10 pm - 5:00 pm",
      "speaker": "Bin Yu",
      "affiliation": "UC Berkeley",
      "abstract": "In this talk, I will introduce the Predictability-Computability-Stability (PCS) framework for veridical (truthful) data science, highlighting its critical role in producing reliable and actionable insights. I will share PCS success stories from cancer detection and cardiology, showcasing how PCS principles have guided cost effective designs and improved outcomes in these projects. Since trustworthy uncertainty quantification is indispensable for cost-reduction in down-stream experiments, I will discuss PCS uncertainty quantification for prediction in regression and multi-class classification. PCS-UQ consists of three steps: pred-check, bootstrap, and multiplicative calibration. Through test results over 26 benchmark datasets, PCS-UQ will be shown to outperform common forms of conformal prediction in terms of width, subgroup coverage, and subgroup interval width. Finally, the multiplicative step in PCS-UQ will be shown to be a new form of conformal prediction.",
      "bio": "Bin Yu is CDSS Chancellor's Distinguished Professor in Statistics, EECS, Center for Computational Biology, and Senior Advisor at the Simons Institute for the Theory of Computing, all at UC Berkeley. Her research focuses on the practice and theory of statistical machine learning, veridical data science, responsible and safe AI, and solving interdisciplinary data problems in neuroscience, genomics, and precision medicine. She and her team have developed algorithms such as iterative random forests (iRF), stability-driven NMF, adaptive wavelet distillation (AWD), Contextual Decomposition for Transformers (CD-T), SPEX and ProxySPEX for interpreting deep learning models, especially for compositional interpretability.\nShe is a member of the National Academy of Sciences and of the American Academy of Arts and Sciences. She was a Guggenheim Fellow, President of Institute of Mathematical Statistics (IMS), and delivered the Tukey Lecture of the Bernoulli Society, the IMS Rietz and Wald Lectures, and Distinguished Achievement Award and Lecture (formerly Fisher Lecture) of COPSS (Committee of Presidents of Statistical Societies). She holds an Honorary Doctorate from The University of Lausanne. She is on the Editorial Board of Proceedings of National Academy of Science (PNAS) and a co-editor of the Harvard Data Science Review (HDSR)."
    },
    {
      "seminar_title": "Beyond Exact Sparsity: Minimax Learning Under Approximate Models",
      "date": "29-Sept-25",
      "location": "Room 903 SSW, 1255 Amsterdam Avenue",
      "time": "4:10 pm - 5:00 pm",
      "speaker": "Jelena Bradic",
      "affiliation": "Cornell University",
      "abstract": "Classical high-dimensional statistics often assumes that models are exactly sparse: only a handful of coefficients matter, and the rest vanish. But what happens when reality is messier? Many modern problems, from nonparametric regression with splines to kernel methods, fit data only approximately through linear structures. In this talk, we’ll explore how moving from exact to approximate sparsity reshapes what is statistically possible.\nI’ll introduce a new framework that extends minimax semiparametric theory to this setting, revealing surprising insights: when root-n estimation is still achievable, when “double robustness” breaks down, and how the rules differ depending on whether regressors are ordered or unordered. Along the way, we’ll see how these results challenge long-standing intuitions about sparsity, efficiency, and optimality.",
      "bio": "Before joining Cornell University, she held positions at the University of California, San Diego, in both the Department of Mathematics and the Halıcıoğlu Data Science Institute. She earned her Ph.D. in Statistics from the Department of Operations Research and Financial Engineering at Princeton University, where she studied under Professor Jianqing Fan. Prior to that, she completed her B.S. and M.S. degrees in Mathematics at the University of Belgrade in Serbia."
    },
    {
      "seminar_title": "Towards Interpretable and Trustworthy Network-Assisted Prediction",
      "date": "6-Oct-25",
      "location": "Room 903 SSW, 1255 Amsterdam Avenue",
      "time": "4:10 pm - 5:00 pm",
      "speaker": "Liza Levina",
      "affiliation": "University of Michigan",
      "abstract": "When training data points for a prediction algorithm are connected by a network, it creates dependency, which reduces effective sample size but also creates an opportunity to improve prediction by leveraging information from neighbors. Multiple prediction methods on networks taking advantage of this opportunity have been developed, but they are rarely interpretable or have uncertainty measures available. This talk will cover two contributions bridging this gap. One is a conformal prediction method for network-assisted regression. The other is a family of flexible network-assisted models built upon a generalization of random forests (RF+), which both achieves competitive prediction accuracy and can be interpreted through feature importance measures. Importantly, it allows one to separate the importance of node covariates in prediction from the importance of the network itself. These tools help broaden the scope and applicability of network-assisted prediction to practical applications.\nThis talk is based on joint work with Robert Lunde, Tiffany Tang, and Ji Zhu.",
      "bio": "Liza Levina is the Vijay Nair Collegiate Professor of Statistics at the University of Michigan, and affiliated faculty at the Michigan Institute for Data and AI in Society and the Center for the Study of Complex Systems. She received her PhD in Statistics from UC Berkeley in 2002, and has been at the University of Michigan since, serving as the department chair from 2020 to 2025. Her research interests are in network analysis, high-dimensional statistics, statistical learning, and applications to neuroscience and imaging. Honors include a fellow of the American Statistical Association, a fellow of the Institute of Mathematical Statistics, a Web of Science Highly Cited Researcher, an IMS Medallion lecturer, and an ICM invited speaker."
    },
    {
      "seminar_title": "Robust generalization and transfer learning with sensitivity analysis",
      "date": "13-Oct-25",
      "location": "Room 903 SSW, 1255 Amsterdam Avenue",
      "time": "4:10 pm - 5:00 pm",
      "speaker": "Hyunseung Kang",
      "affiliation": "UW Madison",
      "abstract": "In recent years, a popular task in data science has been to learn a feature of a new, target distribution Q, denoted as f(Q), using an existing dataset from a source distribution P. This task, broadly referred to as generalization or transfer learning, relies on a key assumption called conditional exchangeability. Unfortunately, this assumption is often untenable in practice and worse, the assumption cannot be verified with data.\nThe main theme of the talk is to study the sensitivity of learning about f(Q) when conditional exchangeability is violated. The first part of the talk presents how to conduct nonparametric and efficient inference for f(Q) under a well-known sensitivity model from causal inference. We also propose a method to benchmark or calibrate the sensitivity parameter using an idea from design sensitivity by Rosenbaum (2004). The second part of the talk proposes a novel measure to assess the sensitivity of learning f(Q) under local violations of conditional exchangeability. The measure, which we call SLOPE, is inspired by an idea from Hampel (1974)’s influence curve and sensitivity analysis from causal inference. Practically, SLOPE helps investigators address questions about which datasets to use for generalization and broadly reflects a philosophy that robust generalization should start with a robust study design.\nThis work is joint work with Xinran Miao (UW-Madison) and Jiwei Zhao (UW-Madison).",
      "bio": "Hyunseung (pronounced Hun-Sung) is an Associate Professor in the Department of Statistics at the University of Wisconsin–Madison. In 2015, he received his Ph.D. in Statistics from the University of Pennsylvania, where he was advised by Tony Cai and Dylan Small. From 2015 to 2016, he completed his NSF postdoctoral fellowship under Guido Imbens at Stanford University, and he has been at Madison since 2017. His research focuses on developing methods for causal inference, with particular emphasis on (a) instrumental variables and unmeasured confounding, (b) semi/nonparametric inference, and (c) dependence. He is also interested in applications to genetics, education, political science, and applied microeconomics."
    },
    {
      "seminar_title": "Causal Inference in Dynamic Thresholding Designs",
      "date": "20-Oct-25",
      "location": "Room 903 SSW, 1255 Amsterdam Avenue",
      "time": "4:10 pm - 5:00 pm",
      "speaker": "Stefan Wager",
      "affiliation": "Stanford University",
      "abstract": "Consider a setting where we regularly monitor patients’ fasting blood sugar, and declare them to have prediabetes (and encourage lifestyle changes) if this number crosses a pre-specified threshold. The sharp, threshold-based treatment policy suggests that we should be able to estimate the long-term benefit of the lifestyle interventions given to prediabetic patients by comparing the health trajectories of patients with blood sugar measurements right above and below the threshold. A naive regression-discontinuity analysis, however, is not applicable here, as it ignores the temporal dynamics of the problem where, e.g., a patient just below the threshold on one visit may become prediabetic (and receive treatment) following their next visit. Here, we study dynamic thresholding designs in Markovian systems with partially observed states, and show that a regression-discontinuity estimator run on aggregate discounted outcomes can still be used to identify a relevant causal target, namely the policy gradient of moving the treatment threshold. We develop results for estimation and inference of this target and discuss implications of our findings for the interpretation of regression discontinuity studies in preventive healthcare. More broadly, our results highlight the promise of adapting widely used observational study techniques to dynamic systems. Joint work with Aditya Ghosh.",
      "bio": "I am an associate professor of Operations, Information, and Technology at the Stanford Graduate School of Business, and an associate professor of Statistics (by courtesy). My research lies at the intersection of causal inference, optimization, and statistical learning. I am particularly interested in developing new solutions to problems in statistics, economics and decision making that leverage recent advances in machine learning."
    },
    {
      "seminar_title": "Higher order influence functions and the inadmissibility of double machine learning (DML) estimators under an assumption lean model",
      "date": "27-Oct-25",
      "location": "Room 903 SSW, 1255 Amsterdam Avenue",
      "time": "4:10 pm - 5:00 pm",
      "speaker": "Jamie Robins",
      "affiliation": "Harvard University",
      "abstract": "For many functionals, DML estimators are the state-of-the-art, incorporating the good predictive performance of black-box machine learning algorithms; the decreased bias of doubly robust estimators; and the analytic tractability and bias reduction of sample splitting with cross fitting. Recently Balakrishnan, Wasserman and Kennedy (BWK) introduced a novel assumption-lean model that formalizes the problem of functional estimation when no complexity reducing assumptions (such as smoothness or sparsity) are imposed on the nuisance functions occurring in the functional’s first order influence function (IF 1 ). They showed that, under BWK assumption- lean model, DML estimators are rate minimax for the integrated squared density and the expected conditional variance functionals. However, earlier Liu, Mukherjee, and Robins (2020) had shown that, for these functionals, higher-order influence function (HOIF) based estimators (ie estimators that add a debiasing mth-order U-statistic to a DML estimator) could have smaller risk (mean squared error) than the DML estimator.\nIn this talk, I resolve this apparent paradox. I show that, although minimax, DML estimators of these functionals are (asymptotically) inadmissible under the BWK model because the risk of any DML estimator is never less than that of the corresponding HOIF estimator and, under many laws, can be much greater. As a consequence, under many data generating laws, HOIF estimators can be used to show that actual coverage of nominal 1-alpha Wald confidence intervals centered at a DML estimator is less than nominal.\nThis work is joint with Lin Liu and Rajarshi Mukherjeee",
      "bio": "Jamie Robins is the Mitchell L. & Robin LaFoley Dong Professor of Epidemiology at the Harvard School of Public Health. He is widely recognized as one of the pioneers of causal inference. His work spans the entire spectrum of causal inference, from dynamic treatment regimes, the g-formula, and double robustness to higher order influence functions and target trials. He is the recipient of the 2022 Rousseeuw Prize for Statistics and 2025 COPSS Distinguished Achievement Award."
    },
    {
      "seminar_title": "NO SEMINAR (HOLIDAY)",
      "date": "3-Nov-25",
      "location": "Room 903 SSW, 1255 Amsterdam Avenue",
      "time": "4:10 pm - 5:00 pm",
      "speaker": "N/A",
      "affiliation": "N/A",
      "abstract": "N/A",
      "bio": "N/A"
    },
    {
      "seminar_title": "Bayesian inference for tail risk extrapolation in time series",
      "date": "10-Nov-25",
      "location": "Room 903 SSW, 1255 Amsterdam Avenue",
      "time": "4:10 pm - 5:00 pm",
      "speaker": "Simone Padoan",
      "affiliation": "Bocconi",
      "abstract": "Temporal dependence is common in many applications, from financial time series to those in climate science, and it affects the behavior of extremes by inducing clustering. As a result, time series data contain less effective information than independent samples, leading to greater uncertainty in extreme value analysis. In this work, we study the Peaks-Over-Threshold (POT) framework, the most widely used method for modeling and estimating univariate extremes, within (strictly) stationary time series under weak mixing conditions. We propose a Bayesian inferential procedure based on the Generalized Pareto (GP) model, specifically tailored to time series settings, which enables extrapolation of both marginal (pertaining to the stationary distribution) and dynamic (conditional on the past) tail risks. We investigate the theoretical accuracy guarantees of our approach and to do so we derived some global properties of the underlying likelihood that had not been previously analyzed. Extensive simulations and real-data applications demonstrate the good inferential performance and practical relevance of the proposed methodology.",
      "bio": "I am an Associate Professor at the department of Decision Sciences at Bocconi University. My research fellow at the Euro-Mediterranean Center on Climate Change (CMCC - Entro Euro Mediterraneo sui Cambiamenti Climatici). Previously I have been an assistant professor in the Department of Decision Sciences at Bocconi University. I received my PhD in Statistical Science in 2008 from the University of Padua, Italy. From 2008 to 2010 I was a post-doc researcher of statistics at Ecole Polytechnique Fédérale de Lausanne, in Lausanne, Switzerland. Editorial activities: Stochastic Models (since 2019) and Extremes (since 2019). My scientific interests are in statistics and applied probability in general, with special emphasis on extreme-value theory, statistics of extremes, nonparametric statistics, Bayesian statistics, time series, spatial statistics, computational statistics, environmental and climate analysis, financial and economic analysis, data science."
    },
    {
      "seminar_title": "Interpolation - based learning under data heterogeneity.",
      "date": "17-Nov-25",
      "location": "Room 903 SSW, 1255 Amsterdam Avenue",
      "time": "4:10 pm - 5:00 pm",
      "speaker": "Sohom Bhattacharya",
      "affiliation": "University of Florida",
      "abstract": "Min-norm interpolators naturally arise as implicitly regularized limits of modern neural networks and other widely used algorithms. While their out-of-distribution risk has recently been analyzed in settings where test samples are unavailable during training, many practical applications allow partial access to target data. The behavior of min-norm interpolation in such settings remains poorly understood. In this talk, I will present a sharp characterization of the risk of min-norm interpolators under both covariate and model shifts. Our results yield several important insights. For instance, in the presence of model shift, incorporating additional data can actually harm prediction performance when the signal-to-noise ratio is low. Conversely, for higher signal-to-noise ratios, transfer learning is beneficial—provided the shift-to-signal ratio remains below a precise threshold, which I will define. To reach these conclusions, we develop novel anisotropic local laws, representing new advances in random matrix theory for heterogeneous data problems. I will also discuss applications of our results to the challenge of combining real data with synthetic data generated by AI models.\nThis talk is based on joint works with Anvit Garg, Kenny Gu, Yanke Song, Debarghya Mukherjee, and Pragya Sur.",
      "bio": "Sohom Bhattacharya is an Assistant Professor in the Department of Statistics at the University of Florida. Before joining UF, he was a Postdoctoral Research Associate in the Department of Operations Research and Financial Engineering at Princeton University. He received his Ph.D. in Statistics from Stanford University in 2022. His research interests lie at the intersection of high-dimensional statistics and statistical machine learning. He has also worked in applied probability, focusing on random graphs and inference in networks."
    },
    {
      "seminar_title": "Observations on Clustering in High Dimensions",
      "date": "24-Nov-25",
      "location": "Room 903 SSW, 1255 Amsterdam Avenue",
      "time": "4:10 pm - 5:00 pm",
      "speaker": "Roy Lederman",
      "affiliation": "Yale",
      "abstract": "TBA",
      "bio": "I am an Assistant Professor at the Department of Statistics and Data Science at Yale University. I am also a member of the Quantitative Biology Institute (QBio), the Applied Math Program, the Institute for Foundations of Data Science (FDS) and the Wu Tsai Institute (WTI) at Yale. I am a Sloan Research Fellow (2023)."
    },
    {
      "seminar_title": "From Score Estimation to Sampling",
      "date": "1-Dec-25",
      "location": "Room 903 SSW, 1255 Amsterdam Avenue",
      "time": "4:10 pm - 5:00 pm",
      "speaker": "Harrison Zhou",
      "affiliation": "Yale",
      "abstract": "Recent advances in the algorithmic generation of high-fidelity images, audio, and video have been largely driven by the success of score-based diffusion models. A key component in their implementation is score matching, which estimates the score function of the forward diffusion process from training data. In this work, we establish rate-optimal procedures for estimating the score function of smooth, compactly supported densities and explore their implications for density estimation and optimal transport.",
      "bio": "Harrison Zhou is the Henry Ford II Professor of Statistics and Data Science at Yale University. He earned his Ph.D. in Mathematics from Cornell University in 2004. A leading scholar in the statistical decision theory, Zhou is known for his work on the fundamental limits of statistical estimation. He currently serves as an Editor-in-Chief of The Annals of Statistics. During his tenure as department chair at Yale, he played a pivotal role in transforming the Department of Statistics into a full-fledged Data Science Department."
    },
    {
      "seminar_title": "NO SEMINAR",
      "date": "8-Dec-25",
      "location": "Room 903 SSW, 1255 Amsterdam Avenue",
      "time": "4:10 pm - 5:00 pm",
      "speaker": "N/A",
      "affiliation": "N/A",
      "abstract": "N/A",
      "bio": "N/A"
    }
  ]
}