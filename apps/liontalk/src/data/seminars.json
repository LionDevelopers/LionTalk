[
  {
    "department": "Statistics",
    "entries": [
      {
        "seminar_title": "Empirical Explorations of Optimization and Generalization for Neural Nets",
        "date": "8-Sept-25",
        "location": "Room 903 SSW, 1255 Amsterdam Avenue",
        "time": "4:10 pm - 5:00 pm",
        "speaker": "Micah Goldblum",
        "affiliation": "Columbia University",
        "abstract": "The fundamental building blocks of machine learning are (1) optimization or fitting the training data and (2) generalization or extrapolating from the training data to unseen samples. In this talk, we will explore both of these topics in the context of neural networks. First, we will inspect neural network loss functions and what they can tell us about generalization. Then, we will discuss recent work on stabilizing optimization for language models. We show that vanilla SGD without momentum is nearly as fast as Adam in the small-batch regime.",
        "bio": "(goldblum.github.io/): Micah Goldblum is an assistant professor in the department of electrical engineering at Columbia University. Before his current position, Micah was a postdoctoral researcher at New York University with Yann LeCun and Andrew Gordon Wilson. Micah\u2019s research focuses on both applied and fundamental problems in machine learning including AI safety, automated data science, training and inference strategies for large-scale models, and building a mathematical and also scientific understanding of why complex AI systems work."
      },
      {
        "seminar_title": "Tailoring matrices to boost spectral algorithms",
        "date": "15-Sept-25",
        "location": "Room 903 SSW, 1255 Amsterdam Avenue",
        "time": "4:10 pm - 5:00 pm",
        "speaker": "Tim Kunisky",
        "affiliation": "Johns Hopkins",
        "abstract": "Spectral algorithms, ones performing hypothesis testing or estimation by looking at the eigenvalues and eigenvectors of a suitable matrix, are one of the central computational techniques of high-dimensional statistics. They enjoy the advantages of being simple to define, efficient, robust, and often straightforward to analyze using the powerful machinery of random matrix theory. However, this talk will show that, in several widely studied settings, the \"naive\" spectral algorithm one might naturally reach for is suboptimal, and can be improved on by choosing a subtler matrix (or matrices) whose spectrum to study.\nI will present two illustrative vignettes. In the planted clique problem over graphs, the natural spectral algorithm applied to the adjacency matrix can be surpassed by instead working with a \"nonlinear Laplacian\", a linear combination of the adjacency matrix and a nonlinear function of the diagonal degree matrix. Similarly, in the synchronization problem over finite or compact groups, various ad hoc spectral methods are inferior to combining spectral information from several matrices chosen according to the representation theory of the underlying group. Through these examples, I will argue that much remains to be understood about spectral algorithms: even in these basic models and for algorithms just a step beyond the simplest ones, numerous subtle phenomena of random matrix theory arise, and optimally tuning these more flexible algorithms already appears to be a task currently amenable only to heuristics and numerical experiments. Finally, time permitting, I will propose a broader research program of using equivariant matrix-to-matrix functions to systematically design matrices for spectral methods and discuss some of the theoretical challenges this direction poses.\nBased on joint work with Yuxin Ma and Shujing Chen.",
        "bio": "Tim Kunisky is an Assistant Professor in the Department of Applied Mathematics and Statistics at Johns Hopkins University."
      },
      {
        "seminar_title": "Veridical Data Science towards Cost-Effective Hypothesis Generation in Science",
        "date": "24-Sept-25",
        "location": "Room 903 SSW, 1255 Amsterdam Avenue",
        "time": "4:10 pm - 5:00 pm",
        "speaker": "Bin Yu",
        "affiliation": "UC Berkeley",
        "abstract": "In this talk, I will introduce the Predictability-Computability-Stability (PCS) framework for veridical (truthful) data science, highlighting its critical role in producing reliable and actionable insights. I will share PCS success stories from cancer detection and cardiology, showcasing how PCS principles have guided cost effective designs and improved outcomes in these projects. Since trustworthy uncertainty quantification is indispensable for cost-reduction in down-stream experiments, I will discuss PCS uncertainty quantification for prediction in regression and multi-class classification. PCS-UQ consists of three steps: pred-check, bootstrap, and multiplicative calibration. Through test results over 26 benchmark datasets, PCS-UQ will be shown to outperform common forms of conformal prediction in terms of width, subgroup coverage, and subgroup interval width. Finally, the multiplicative step in PCS-UQ will be shown to be a new form of conformal prediction.",
        "bio": "Bin Yu is CDSS Chancellor's Distinguished Professor in Statistics, EECS, Center for Computational Biology, and Senior Advisor at the Simons Institute for the Theory of Computing, all at UC Berkeley. Her research focuses on the practice and theory of statistical machine learning, veridical data science, responsible and safe AI, and solving interdisciplinary data problems in neuroscience, genomics, and precision medicine. She and her team have developed algorithms such as iterative random forests (iRF), stability-driven NMF, adaptive wavelet distillation (AWD), Contextual Decomposition for Transformers (CD-T), SPEX and ProxySPEX for interpreting deep learning models, especially for compositional interpretability.\nShe is a member of the National Academy of Sciences and of the American Academy of Arts and Sciences. She was a Guggenheim Fellow, President of Institute of Mathematical Statistics (IMS), and delivered the Tukey Lecture of the Bernoulli Society, the IMS Rietz and Wald Lectures, and Distinguished Achievement Award and Lecture (formerly Fisher Lecture) of COPSS (Committee of Presidents of Statistical Societies). She holds an Honorary Doctorate from The University of Lausanne. She is on the Editorial Board of Proceedings of National Academy of Science (PNAS) and a co-editor of the Harvard Data Science Review (HDSR)."
      },
      {
        "seminar_title": "Beyond Exact Sparsity: Minimax Learning Under Approximate Models",
        "date": "29-Sept-25",
        "location": "Room 903 SSW, 1255 Amsterdam Avenue",
        "time": "4:10 pm - 5:00 pm",
        "speaker": "Jelena Bradic",
        "affiliation": "Cornell University",
        "abstract": "Classical high-dimensional statistics often assumes that models are exactly sparse: only a handful of coefficients matter, and the rest vanish. But what happens when reality is messier? Many modern problems, from nonparametric regression with splines to kernel methods, fit data only approximately through linear structures. In this talk, we\u2019ll explore how moving from exact to approximate sparsity reshapes what is statistically possible.\nI\u2019ll introduce a new framework that extends minimax semiparametric theory to this setting, revealing surprising insights: when root-n estimation is still achievable, when \u201cdouble robustness\u201d breaks down, and how the rules differ depending on whether regressors are ordered or unordered. Along the way, we\u2019ll see how these results challenge long-standing intuitions about sparsity, efficiency, and optimality.",
        "bio": "Before joining Cornell University, she held positions at the University of California, San Diego, in both the Department of Mathematics and the Hal\u0131c\u0131o\u011flu Data Science Institute. She earned her Ph.D. in Statistics from the Department of Operations Research and Financial Engineering at Princeton University, where she studied under Professor Jianqing Fan. Prior to that, she completed her B.S. and M.S. degrees in Mathematics at the University of Belgrade in Serbia."
      },
      {
        "seminar_title": "Towards Interpretable and Trustworthy Network-Assisted Prediction",
        "date": "6-Oct-25",
        "location": "Room 903 SSW, 1255 Amsterdam Avenue",
        "time": "4:10 pm - 5:00 pm",
        "speaker": "Liza Levina",
        "affiliation": "University of Michigan",
        "abstract": "When training data points for a prediction algorithm are connected by a network, it creates dependency, which reduces effective sample size but also creates an opportunity to improve prediction by leveraging information from neighbors. Multiple prediction methods on networks taking advantage of this opportunity have been developed, but they are rarely interpretable or have uncertainty measures available. This talk will cover two contributions bridging this gap. One is a conformal prediction method for network-assisted regression. The other is a family of flexible network-assisted models built upon a generalization of random forests (RF+), which both achieves competitive prediction accuracy and can be interpreted through feature importance measures. Importantly, it allows one to separate the importance of node covariates in prediction from the importance of the network itself. These tools help broaden the scope and applicability of network-assisted prediction to practical applications.\nThis talk is based on joint work with Robert Lunde, Tiffany Tang, and Ji Zhu.",
        "bio": "Liza Levina is the Vijay Nair Collegiate Professor of Statistics at the University of Michigan, and affiliated faculty at the Michigan Institute for Data and AI in Society and the Center for the Study of Complex Systems. She received her PhD in Statistics from UC Berkeley in 2002, and has been at the University of Michigan since, serving as the department chair from 2020 to 2025. Her research interests are in network analysis, high-dimensional statistics, statistical learning, and applications to neuroscience and imaging. Honors include a fellow of the American Statistical Association, a fellow of the Institute of Mathematical Statistics, a Web of Science Highly Cited Researcher, an IMS Medallion lecturer, and an ICM invited speaker."
      },
      {
        "seminar_title": "Robust generalization and transfer learning with sensitivity analysis",
        "date": "13-Oct-25",
        "location": "Room 903 SSW, 1255 Amsterdam Avenue",
        "time": "4:10 pm - 5:00 pm",
        "speaker": "Hyunseung Kang",
        "affiliation": "UW Madison",
        "abstract": "In recent years, a popular task in data science has been to learn a feature of a new, target distribution Q, denoted as f(Q), using an existing dataset from a source distribution P. This task, broadly referred to as generalization or transfer learning, relies on a key assumption called conditional exchangeability. Unfortunately, this assumption is often untenable in practice and worse, the assumption cannot be verified with data.\nThe main theme of the talk is to study the sensitivity of learning about f(Q) when conditional exchangeability is violated. The first part of the talk presents how to conduct nonparametric and efficient inference for f(Q) under a well-known sensitivity model from causal inference. We also propose a method to benchmark or calibrate the sensitivity parameter using an idea from design sensitivity by Rosenbaum (2004). The second part of the talk proposes a novel measure to assess the sensitivity of learning f(Q) under local violations of conditional exchangeability. The measure, which we call SLOPE, is inspired by an idea from Hampel (1974)\u2019s influence curve and sensitivity analysis from causal inference. Practically, SLOPE helps investigators address questions about which datasets to use for generalization and broadly reflects a philosophy that robust generalization should start with a robust study design.\nThis work is joint work with Xinran Miao (UW-Madison) and Jiwei Zhao (UW-Madison).",
        "bio": "Hyunseung (pronounced Hun-Sung) is an Associate Professor in the Department of Statistics at the University of Wisconsin\u2013Madison. In 2015, he received his Ph.D. in Statistics from the University of Pennsylvania, where he was advised by Tony Cai and Dylan Small. From 2015 to 2016, he completed his NSF postdoctoral fellowship under Guido Imbens at Stanford University, and he has been at Madison since 2017. His research focuses on developing methods for causal inference, with particular emphasis on (a) instrumental variables and unmeasured confounding, (b) semi/nonparametric inference, and (c) dependence. He is also interested in applications to genetics, education, political science, and applied microeconomics."
      },
      {
        "seminar_title": "Causal Inference in Dynamic Thresholding Designs",
        "date": "20-Oct-25",
        "location": "Room 903 SSW, 1255 Amsterdam Avenue",
        "time": "4:10 pm - 5:00 pm",
        "speaker": "Stefan Wager",
        "affiliation": "Stanford University",
        "abstract": "Consider a setting where we regularly monitor patients\u2019 fasting blood sugar, and declare them to have prediabetes (and encourage lifestyle changes) if this number crosses a pre-specified threshold. The sharp, threshold-based treatment policy suggests that we should be able to estimate the long-term benefit of the lifestyle interventions given to prediabetic patients by comparing the health trajectories of patients with blood sugar measurements right above and below the threshold. A naive regression-discontinuity analysis, however, is not applicable here, as it ignores the temporal dynamics of the problem where, e.g., a patient just below the threshold on one visit may become prediabetic (and receive treatment) following their next visit. Here, we study dynamic thresholding designs in Markovian systems with partially observed states, and show that a regression-discontinuity estimator run on aggregate discounted outcomes can still be used to identify a relevant causal target, namely the policy gradient of moving the treatment threshold. We develop results for estimation and inference of this target and discuss implications of our findings for the interpretation of regression discontinuity studies in preventive healthcare. More broadly, our results highlight the promise of adapting widely used observational study techniques to dynamic systems. Joint work with Aditya Ghosh.",
        "bio": "I am an associate professor of Operations, Information, and Technology at the Stanford Graduate School of Business, and an associate professor of Statistics (by courtesy). My research lies at the intersection of causal inference, optimization, and statistical learning. I am particularly interested in developing new solutions to problems in statistics, economics and decision making that leverage recent advances in machine learning."
      },
      {
        "seminar_title": "Higher order influence functions and the inadmissibility of double machine learning (DML) estimators under an assumption lean model",
        "date": "27-Oct-25",
        "location": "Room 903 SSW, 1255 Amsterdam Avenue",
        "time": "4:10 pm - 5:00 pm",
        "speaker": "Jamie Robins",
        "affiliation": "Harvard University",
        "abstract": "For many functionals, DML estimators are the state-of-the-art, incorporating the good predictive performance of black-box machine learning algorithms; the decreased bias of doubly robust estimators; and the analytic tractability and bias reduction of sample splitting with cross fitting. Recently Balakrishnan, Wasserman and Kennedy (BWK) introduced a novel assumption-lean model that formalizes the problem of functional estimation when no complexity reducing assumptions (such as smoothness or sparsity) are imposed on the nuisance functions occurring in the functional\u2019s first order influence function (IF 1 ). They showed that, under BWK assumption- lean model, DML estimators are rate minimax for the integrated squared density and the expected conditional variance functionals. However, earlier Liu, Mukherjee, and Robins (2020) had shown that, for these functionals, higher-order influence function (HOIF) based estimators (ie estimators that add a debiasing mth-order U-statistic to a DML estimator) could have smaller risk (mean squared error) than the DML estimator.\nIn this talk, I resolve this apparent paradox. I show that, although minimax, DML estimators of these functionals are (asymptotically) inadmissible under the BWK model because the risk of any DML estimator is never less than that of the corresponding HOIF estimator and, under many laws, can be much greater. As a consequence, under many data generating laws, HOIF estimators can be used to show that actual coverage of nominal 1-alpha Wald confidence intervals centered at a DML estimator is less than nominal.\nThis work is joint with Lin Liu and Rajarshi Mukherjeee",
        "bio": "Jamie Robins is the Mitchell L. & Robin LaFoley Dong Professor of Epidemiology at the Harvard School of Public Health. He is widely recognized as one of the pioneers of causal inference. His work spans the entire spectrum of causal inference, from dynamic treatment regimes, the g-formula, and double robustness to higher order influence functions and target trials. He is the recipient of the 2022 Rousseeuw Prize for Statistics and 2025 COPSS Distinguished Achievement Award."
      },
      {
        "seminar_title": "NO SEMINAR (HOLIDAY)",
        "date": "3-Nov-25",
        "location": "Room 903 SSW, 1255 Amsterdam Avenue",
        "time": "4:10 pm - 5:00 pm",
        "speaker": "N/A",
        "affiliation": "N/A",
        "abstract": "N/A",
        "bio": "N/A"
      },
      {
        "seminar_title": "Bayesian inference for tail risk extrapolation in time series",
        "date": "10-Nov-25",
        "location": "Room 903 SSW, 1255 Amsterdam Avenue",
        "time": "4:10 pm - 5:00 pm",
        "speaker": "Simone Padoan",
        "affiliation": "Bocconi",
        "abstract": "Temporal dependence is common in many applications, from financial time series to those in climate science, and it affects the behavior of extremes by inducing clustering. As a result, time series data contain less effective information than independent samples, leading to greater uncertainty in extreme value analysis. In this work, we study the Peaks-Over-Threshold (POT) framework, the most widely used method for modeling and estimating univariate extremes, within (strictly) stationary time series under weak mixing conditions. We propose a Bayesian inferential procedure based on the Generalized Pareto (GP) model, specifically tailored to time series settings, which enables extrapolation of both marginal (pertaining to the stationary distribution) and dynamic (conditional on the past) tail risks. We investigate the theoretical accuracy guarantees of our approach and to do so we derived some global properties of the underlying likelihood that had not been previously analyzed. Extensive simulations and real-data applications demonstrate the good inferential performance and practical relevance of the proposed methodology.",
        "bio": "I am an Associate Professor at the department of Decision Sciences at Bocconi University. My research fellow at the Euro-Mediterranean Center on Climate Change (CMCC - Entro Euro Mediterraneo sui Cambiamenti Climatici). Previously I have been an assistant professor in the Department of Decision Sciences at Bocconi University. I received my PhD in Statistical Science in 2008 from the University of Padua, Italy. From 2008 to 2010 I was a post-doc researcher of statistics at Ecole Polytechnique F\u00e9d\u00e9rale de Lausanne, in Lausanne, Switzerland. Editorial activities: Stochastic Models (since 2019) and Extremes (since 2019). My scientific interests are in statistics and applied probability in general, with special emphasis on extreme-value theory, statistics of extremes, nonparametric statistics, Bayesian statistics, time series, spatial statistics, computational statistics, environmental and climate analysis, financial and economic analysis, data science."
      },
      {
        "seminar_title": "Interpolation - based learning under data heterogeneity.",
        "date": "17-Nov-25",
        "location": "Room 903 SSW, 1255 Amsterdam Avenue",
        "time": "4:10 pm - 5:00 pm",
        "speaker": "Sohom Bhattacharya",
        "affiliation": "University of Florida",
        "abstract": "Min-norm interpolators naturally arise as implicitly regularized limits of modern neural networks and other widely used algorithms. While their out-of-distribution risk has recently been analyzed in settings where test samples are unavailable during training, many practical applications allow partial access to target data. The behavior of min-norm interpolation in such settings remains poorly understood. In this talk, I will present a sharp characterization of the risk of min-norm interpolators under both covariate and model shifts. Our results yield several important insights. For instance, in the presence of model shift, incorporating additional data can actually harm prediction performance when the signal-to-noise ratio is low. Conversely, for higher signal-to-noise ratios, transfer learning is beneficial\u2014provided the shift-to-signal ratio remains below a precise threshold, which I will define. To reach these conclusions, we develop novel anisotropic local laws, representing new advances in random matrix theory for heterogeneous data problems. I will also discuss applications of our results to the challenge of combining real data with synthetic data generated by AI models.\nThis talk is based on joint works with Anvit Garg, Kenny Gu, Yanke Song, Debarghya Mukherjee, and Pragya Sur.",
        "bio": "Sohom Bhattacharya is an Assistant Professor in the Department of Statistics at the University of Florida. Before joining UF, he was a Postdoctoral Research Associate in the Department of Operations Research and Financial Engineering at Princeton University. He received his Ph.D. in Statistics from Stanford University in 2022. His research interests lie at the intersection of high-dimensional statistics and statistical machine learning. He has also worked in applied probability, focusing on random graphs and inference in networks."
      },
      {
        "seminar_title": "Observations on Clustering in High Dimensions",
        "date": "24-Nov-25",
        "location": "Room 903 SSW, 1255 Amsterdam Avenue",
        "time": "4:10 pm - 5:00 pm",
        "speaker": "Roy Lederman",
        "affiliation": "Yale",
        "abstract": "TBA",
        "bio": "I am an Assistant Professor at the Department of Statistics and Data Science at Yale University. I am also a member of the Quantitative Biology Institute (QBio), the Applied Math Program, the Institute for Foundations of Data Science (FDS) and the Wu Tsai Institute (WTI) at Yale. I am a Sloan Research Fellow (2023)."
      },
      {
        "seminar_title": "From Score Estimation to Sampling",
        "date": "1-Dec-25",
        "location": "Room 903 SSW, 1255 Amsterdam Avenue",
        "time": "4:10 pm - 5:00 pm",
        "speaker": "Harrison Zhou",
        "affiliation": "Yale",
        "abstract": "Recent advances in the algorithmic generation of high-fidelity images, audio, and video have been largely driven by the success of score-based diffusion models. A key component in their implementation is score matching, which estimates the score function of the forward diffusion process from training data. In this work, we establish rate-optimal procedures for estimating the score function of smooth, compactly supported densities and explore their implications for density estimation and optimal transport.",
        "bio": "Harrison Zhou is the Henry Ford II Professor of Statistics and Data Science at Yale University. He earned his Ph.D. in Mathematics from Cornell University in 2004. A leading scholar in the statistical decision theory, Zhou is known for his work on the fundamental limits of statistical estimation. He currently serves as an Editor-in-Chief of The Annals of Statistics. During his tenure as department chair at Yale, he played a pivotal role in transforming the Department of Statistics into a full-fledged Data Science Department."
      },
      {
        "seminar_title": "NO SEMINAR",
        "date": "8-Dec-25",
        "location": "Room 903 SSW, 1255 Amsterdam Avenue",
        "time": "4:10 pm - 5:00 pm",
        "speaker": "N/A",
        "affiliation": "N/A",
        "abstract": "N/A",
        "bio": "N/A"
      }
    ]
  },
  {
    "department": "Student",
    "entries": [
      {
        "seminar_title": "Some new insights on Supervised Transfer Learning",
        "date": "17-Sep-25",
        "location": "Room 1025 SSW, 1255 Amsterdam Avenue",
        "time": "12:00 - 1:00 pm",
        "speaker": "Prof. Kpotufe",
        "affiliation": "Columbia University",
        "abstract": "Supervised Transfer Learning (STL) concerns scenarios where a learner has access to some labeled data for a prediction task, a much labeled data from a different but related distribution. The goal of the learner is to optimally leverage both datasets towards improved prediction on the target task. We are interested in the extent to which existing notions of distance or discrepancy between distributions might capture the statistical hardness of this problem, and design efficient procedures that can automatically adapt to a priori unknown discrepancy between distributions.",
        "bio": ""
      },
      {
        "seminar_title": "A cumulant approach to linear regression",
        "date": "24-Sep-25",
        "location": "Room 1025 SSW, 1255 Amsterdam Avenue",
        "time": "12:00 - 1:00 pm",
        "speaker": "Prof. Ming",
        "affiliation": "Columbia University",
        "abstract": "Linear regression is a cornerstone of statistics. Despite its generality and flexibility, its interpretation can be subtle. In this talk I will describe a new perspective to linear regression through the lens of higher order cumulants and discuss how it offers new insights into a number of classical problems including the omitted variable bias.",
        "bio": ""
      },
      {
        "seminar_title": "Buy-side Quantitative Research Landscape",
        "date": "1-Oct-25",
        "location": "Room 1025 SSW, 1255 Amsterdam Avenue",
        "time": "12:00 - 1:00 pm",
        "speaker": "Jeremy Shen",
        "affiliation": "Global Quantitative Strategies",
        "abstract": "We will discuss the general landscape of quantitative research on the buy-side, as well as highly stylized problems one might encounter as a statistician. This will be an interactive & conversational session.",
        "bio": "Jeremy Shen is currently on garden leave from his former post as a Quantitative Research Lead at Citadel Global Quantitative Strategies (GQS), where he led a team that focused on forecasting and trading in global futures & currencies markets. Prior to Citadel, he held systematic macro research roles at Two Sigma Investments. He holds a PhD in Statistics from Stanford University."
      },
      {
        "seminar_title": "STUDENT SEMINAR HAS BEEN CANCELLED!",
        "date": "8-Oct-25",
        "location": "Room 1025 SSW, 1255 Amsterdam Avenue",
        "time": "12:00 - 1:00 pm",
        "speaker": "",
        "affiliation": "",
        "abstract": "",
        "bio": ""
      },
      {
        "seminar_title": "Consistency and Inconsistency in K-Means Clustering",
        "date": "15-Oct-25",
        "location": "Room 1025 SSW, 1255 Amsterdam Avenue",
        "time": "12:00 - 1:00 pm",
        "speaker": "Adam Jaffe",
        "affiliation": "Columbia University",
        "abstract": "A celebrated results of Pollard proves the asymptotic consistency of k-means clustering when the population distribution has finite variance. In this talk, we point out that the population-level k-means clustering problem is, in fact, well-posed whenever the population distribution has finite expectation, and we investigate whether some form of asymptotic consistency holds in this setting. Surprisingly, the answer is no: We construct examples in which there is a unique set of population cluster centers, but where the empirical cluster centers oscillate to plus and minus infinity as the number of data increases. We show that this non-convergence is due to an extreme form of imbalance whereby a few outlying samples create clusters that contain very few points. Based on joint work with Mo\u00efse Blanchard and Nikita Zhivotovskiy.",
        "bio": ""
      },
      {
        "seminar_title": "The Conflict Graph Design: Estimating Causal Effects Under Network Interference",
        "date": "22-Oct-25",
        "location": "Room 1025 SSW, 1255 Amsterdam Avenue",
        "time": "12:00 - 1:00 pm",
        "speaker": "Prof. Chris Harshaw",
        "affiliation": "Columbia University",
        "abstract": "From political science and economics to public health and corporate strategy, the randomized experiment is a widely used methodological tool for estimating causal effects. In the past 15 years or so, there has been a growing interest in network experiments, where subjects are presumed to be interacting in the experiment and their interactions are of substantive interest. While the literature on interference has focused primarily on unbiased and consistent estimation, designing randomized network experiments to ensure tight rates of convergence is relatively under-explored. Not only are the optimal rates of estimation for different causal effects under interference an open question but previously proposed designs are created in an ad-hoc fashion.\nIn this talk, I will present a new experimental design for network experiments called the \"Conflict Graph Design\" which, given a pre-specified causal effect of interest and the underlying network, produces a randomization over treatment assignment with the goal of increasing the precision of effect estimation. Not only does this experiment design attain improved rates of consistency for several causal effects of interest, it also provides a unifying approach to designing network experiments. We also provide consistent variance estimators and asymptotically valid confidence intervals which facilitate inference of the causal effect under investigation.",
        "bio": ""
      },
      {
        "seminar_title": "Computational Lower Bounds for Tensor PCA",
        "date": "29-Oct-25",
        "location": "Room 1025 SSW, 1255 Amsterdam Avenue",
        "time": "12:00 - 1:00 pm",
        "speaker": "Prof. Hsu",
        "affiliation": "Columbia University",
        "abstract": "Tensor PCA is a model statistical inference problem introduced by Montanari and Richard in 2014 for studying method-of-moments approaches to parameter estimation in latent variable models. Unlike the matrix counterpart of the problem, Tensor PCA exhibits a computational-statistical gap in the sample-size regime where the problem is information-theoretically solvable but no computationally-efficient algorithm is known. I will describe unconditional computational lower bounds on classes of algorithms for solving Tensor PCA that shed light on limitations of commonly-used solution approaches, including gradient descent and power iteration, as well as the role of overparameterization. This talk is based on joint work with Rishabh Dudeja.",
        "bio": ""
      },
      {
        "seminar_title": "Conservative Diffusions as Entropic Flows of Steepest Descent",
        "date": "5-Nov-25",
        "location": "Room 1025 SSW, 1255 Amsterdam Avenue",
        "time": "12:00 - 1:00 pm",
        "speaker": "Prof. Ioannis Karatzas",
        "affiliation": "Columbia University",
        "abstract": "We provide a detailed, probabilistic interpretation for the variational characterization of conservative diffusions as entropic flows of steepest descent. Jordan, Kinderlehrer, and Otto showed in 1998, via a numerical scheme, that for diffusions of Langevin-Smoluchowski type the Fokker-Planck probability density flow minimizes the rate of relative entropy dissipation, as measured by the distance traveled in terms of the quadratic Wasserstein metric in the ambient space of configurations. Using a very direct perturbation analysis we obtain novel, stochastic-process versions of such features; these are valid along almost every trajectory of the motion in both the forward and, most transparently, the backward, directions of time. The original results follow then simply by \u201caggregating\u201d, i.e., by taking expectations. As a bonus we obtain a version of the HWI inequality of Otto and Villani, relating relative entropy, Fisher information, and Wasserstein distance.",
        "bio": ""
      },
      {
        "seminar_title": "Wasserstein-Cramer-Rao Theory of Unbiased Estimation",
        "date": "12-Nov-25",
        "location": "Room 1025 SSW, 1255 Amsterdam Avenue",
        "time": "12:00 - 1:00 pm",
        "speaker": "Prof. Bodhi Sen",
        "affiliation": "Columbia University",
        "abstract": "The quantity of interest in the classical Cramer-Rao theory of unbiased estimation (i.e., the Cramer-Rao lower bound, exact efficiency in exponential families, and asymptotic efficiency of maximum likelihood estimation) is the variance, which represents the instability of an estimator when its value is compared to the value for an independently-sampled data set from the same distribution. In this paper we are interested in a quantity which represents the instability of an estimator when its value is compared to the value for an infinitesimal additive perturbation of the original data set; we refer to this as the \"sensitivity\" of an estimator. The resulting theory of sensitivity is based on the Wasserstein geometry in the same way that the classical theory of variance is based on the Fisher-Rao (equivalently, Hellinger) geometry, and this insight allows us to determine a collection of results which are analogous to the classical case: a Wasserstein-Cramer-Rao lower bound for the sensitivity of any unbiased estimator, a characterization of models in which there exist unbiased estimators achieving the lower bound exactly, and a guarantee that Wasserstein projection estimators achieve the lower bound asymptotically. We use these results to treat many statistical examples, sometimes revealing new optimality properties for existing estimators and other times revealing new estimators. This is joint work with Nicolas Garcia Trillos (U Wisconsin) and Adam Jaffe (Columbia).",
        "bio": ""
      },
      {
        "seminar_title": "Variational Deep Learning via Implicit Regularization",
        "date": "19-Nov-25",
        "location": "Room 1025 SSW, 1255 Amsterdam Avenue",
        "time": "12:00 - 1:00 pm",
        "speaker": "Post-Docs - Beau Coker & Jonathan Wenger",
        "affiliation": "Columbia University",
        "abstract": "Modern deep learning models generalize remarkably well in-distribution, despite being overparameterized and trained with little to no explicit regularization. Instead, current theory credits implicit regularization imposed by the choice of architecture, hyperparameters and optimization procedure. However, deep neural networks can be surprisingly non-robust, resulting in overconfident predictions and poor out-of-distribution generalization. Bayesian deep learning addresses this via model averaging, but typically requires significant computational resources as well as carefully elicited priors to avoid overriding the benefits of implicit regularization. In this talk, we introduce a new way to regularize variational neural networks solely by relying on the implicit bias of (stochastic) gradient descent. We theoretically characterize this inductive bias in over parameterized linear models as generalized variational inference and demonstrate the importance of the choice of parametrization. Empirically, our approach demonstrates strong in- and out-of-distribution performance without additional hyperparameter tuning and with minimal computational overhead.",
        "bio": ""
      },
      {
        "seminar_title": "Breakdown point of transport-based quantiles",
        "date": "3-Dec-25",
        "location": "Room 1025 SSW, 1255 Amsterdam Avenue",
        "time": "12:00 - 1:00 pm",
        "speaker": "Prof. Alberto Gonzalez-Sanz",
        "affiliation": "",
        "abstract": "",
        "bio": ""
      },
      {
        "seminar_title": "",
        "date": "10-Dec-25",
        "location": "Room 1025 SSW, 1255 Amsterdam Avenue",
        "time": "12:00 - 1:00 pm",
        "speaker": "",
        "affiliation": "",
        "abstract": "",
        "bio": ""
      }
    ]
  },
  {
    "department": "Mathematical Finance",
    "entries": [
      {
        "seminar_title": "The Optimal Mean-Variance Selling Problem with Finite Horizon",
        "date": "29-Jan-26",
        "location": "Room 903, 1255 Amsterdam Ave.",
        "time": "4:10 pm - 5:25 pm",
        "speaker": "Goran Peskir",
        "affiliation": "University of Manchester",
        "abstract": "Imagine an investor who owns a stock that he wishes to sell so as to maximize his return and minimize his risk upon selling. In line with the classic mean-variance analysis, we identify the return with the expectation of the stock price and the risk with the variance of the stock price.\nThe quadratic nonlinearity of the variance then moves the resulting optimal stopping problem outside the scope of the standard/linear optimal stopping theory. Consequently, the results and methods of the standard/linear optimal stopping theory are no longer applicable in this new/nonlinear setting.\nThe solution to the nonlinear problem when the horizon is infinite has been known for some time, however, the method of proof used to solve the nonlinear problem in that case is not applicable in the case when the horizon is finite. The purpose of the talk is to present a new method of proof which solves the nonlinear problem when the horizon is finite. [This is joint work with P. Johnson and J.L. Pedersen]",
        "bio": ""
      },
      {
        "seminar_title": "",
        "date": "5-Feb-26",
        "location": "Room 903, 1255 Amsterdam Ave.",
        "time": "4:10 pm - 5:25 pm",
        "speaker": "Jim Gatheral and Marting Larsson",
        "affiliation": "NYU",
        "abstract": "",
        "bio": ""
      },
      {
        "seminar_title": "",
        "date": "19-Feb-26",
        "location": "Room 903, 1255 Amsterdam Ave.",
        "time": "4:10 pm - 5:25 pm",
        "speaker": "Yucheng Guo",
        "affiliation": "Princeton",
        "abstract": "",
        "bio": ""
      }
    ]
  }
]