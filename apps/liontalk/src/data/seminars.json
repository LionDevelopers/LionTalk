[
  {
    "department": "Statistics",
    "series": "Statistics Seminar Series",
    "entries": [
      {
        "seminar_title": "Algorithmic Stability in Statistical Inference",
        "date": "26-Jan-26",
        "location": "Room 903 SSW, 1255 Amsterdam Avenue",
        "time": "4:10 pm - 5:00 pm",
        "speaker": "Yuetian Luo",
        "affiliation": "Rutgers University",
        "abstract": "Algorithmic stability is a central concept in statistics and learning theory that measures how sensitive an algorithm's output is to small changes in the training data. In this talk, we consider two different perspectives of algorithmic stability in statistical inference. In the first part, we investigate the problem of distribution-free algorithm risk evaluation, uncovering fundamental limitations for answering these questions with limited amounts of data. To navigate the challenge, we will discuss how incorporating an assumption about algorithmic stability might help. In this second part, we reveal that while stability entails desirable properties, it is typically not sufficient on its own for statistical learning. For example, an algorithm that always outputs a constant function is perfectly stable but statistically meaningless. Then we will discuss the trade-offs between stability and accuracy in statistical inference.",
        "bio": "Yuetian Luo is an assistant professor in the Department of Statistics at Rutgers University. Before that, he was a postdoctoral scholar in the Data Science Institute at the University of Chicago, advised by Professor Rina Foygel Barber. He received his Ph.D. in Statistics from the University of Wisconsin-Madison in 2022 under the supervision of Professor Anru Zhang. His research interests lie broadly in distribution-free inference, computational complexity of statistical inference, tensor learning, robust statistics, and non-convex optimization."
      },
      {
        "seminar_title": "",
        "date": "2-Feb-",
        "location": "Room 903 SSW, 1255 Amsterdam Avenue",
        "time": "4:10 pm - 5:00 pm",
        "speaker": "Weng Kee Wong",
        "affiliation": "UCLA",
        "abstract": "",
        "bio": ""
      }
    ]
  },
  {
    "department": "Statistics",
    "series": "Student Seminar Series",
    "entries": [
      {
        "seminar_title": "Some new insights on Supervised Transfer Learning",
        "date": "17-Sep-25",
        "location": "Room 1025 SSW, 1255 Amsterdam Avenue",
        "time": "12:00 - 1:00 pm",
        "speaker": "Prof. Kpotufe",
        "affiliation": "Columbia University",
        "abstract": "Supervised Transfer Learning (STL) concerns scenarios where a learner has access to some labeled data for a prediction task, a much labeled data from a different but related distribution. The goal of the learner is to optimally leverage both datasets towards improved prediction on the target task. We are interested in the extent to which existing notions of distance or discrepancy between distributions might capture the statistical hardness of this problem, and design efficient procedures that can automatically adapt to a priori unknown discrepancy between distributions.",
        "bio": ""
      },
      {
        "seminar_title": "A cumulant approach to linear regression",
        "date": "24-Sep-25",
        "location": "Room 1025 SSW, 1255 Amsterdam Avenue",
        "time": "12:00 - 1:00 pm",
        "speaker": "Prof. Ming",
        "affiliation": "Columbia University",
        "abstract": "Linear regression is a cornerstone of statistics. Despite its generality and flexibility, its interpretation can be subtle. In this talk I will describe a new perspective to linear regression through the lens of higher order cumulants and discuss how it offers new insights into a number of classical problems including the omitted variable bias.",
        "bio": ""
      },
      {
        "seminar_title": "Buy-side Quantitative Research Landscape",
        "date": "1-Oct-25",
        "location": "Room 1025 SSW, 1255 Amsterdam Avenue",
        "time": "12:00 - 1:00 pm",
        "speaker": "Jeremy Shen",
        "affiliation": "Quantitative Research Lead (Global Quantitative Strategies)",
        "abstract": "We will discuss the general landscape of quantitative research on the buy-side, as well as highly stylized problems one might encounter as a statistician. This will be an interactive & conversational session.",
        "bio": "Jeremy Shen is currently on garden leave from his former post as a Quantitative Research Lead at Citadel Global Quantitative Strategies (GQS), where he led a team that focused on forecasting and trading in global futures & currencies markets. Prior to Citadel, he held systematic macro research roles at Two Sigma Investments. He holds a PhD in Statistics from Stanford University."
      },
      {
        "seminar_title": "STUDENT SEMINAR HAS BEEN CANCELLED!",
        "date": "8-Oct-25",
        "location": "Room 1025 SSW, 1255 Amsterdam Avenue",
        "time": "12:00 - 1:00 pm",
        "speaker": "",
        "affiliation": "",
        "abstract": "",
        "bio": ""
      },
      {
        "seminar_title": "Consistency and Inconsistency in K-Means Clustering",
        "date": "15-Oct-25",
        "location": "Room 1025 SSW, 1255 Amsterdam Avenue",
        "time": "12:00 - 1:00 pm",
        "speaker": "Adam Jaffe",
        "affiliation": "Postdoc (Columbia University)",
        "abstract": "A celebrated results of Pollard proves the asymptotic consistency of k-means clustering when the population distribution has finite variance. In this talk, we point out that the population-level k-means clustering problem is, in fact, well-posed whenever the population distribution has finite expectation, and we investigate whether some form of asymptotic consistency holds in this setting. Surprisingly, the answer is no: We construct examples in which there is a unique set of population cluster centers, but where the empirical cluster centers oscillate to plus and minus infinity as the number of data increases. We show that this non-convergence is due to an extreme form of imbalance whereby a few outlying samples create clusters that contain very few points. Based on joint work with Mo\u00efse Blanchard and Nikita Zhivotovskiy.",
        "bio": ""
      },
      {
        "seminar_title": "The Conflict Graph Design: Estimating Causal Effects Under Network Interference",
        "date": "22-Oct-25",
        "location": "Room 1025 SSW, 1255 Amsterdam Avenue",
        "time": "12:00 - 1:00 pm",
        "speaker": "Prof. Chris Harshaw",
        "affiliation": "Columbia University",
        "abstract": "From political science and economics to public health and corporate strategy, the randomized experiment is a widely used methodological tool for estimating causal effects. In the past 15 years or so, there has been a growing interest in network experiments, where subjects are presumed to be interacting in the experiment and their interactions are of substantive interest. While the literature on interference has focused primarily on unbiased and consistent estimation, designing randomized network experiments to ensure tight rates of convergence is relatively under-explored. Not only are the optimal rates of estimation for different causal effects under interference an open question but previously proposed designs are created in an ad-hoc fashion.\n\nIn this talk, I will present a new experimental design for network experiments called the \"Conflict Graph Design\" which, given a pre-specified causal effect of interest and the underlying network, produces a randomization over treatment assignment with the goal of increasing the precision of effect estimation. Not only does this experiment design attain improved rates of consistency for several causal effects of interest, it also provides a unifying approach to designing network experiments. We also provide consistent variance estimators and asymptotically valid confidence intervals which facilitate inference of the causal effect under investigation.",
        "bio": ""
      },
      {
        "seminar_title": "Computational Lower Bounds for Tensor PCA",
        "date": "29-Oct-25",
        "location": "Room 1025 SSW, 1255 Amsterdam Avenue",
        "time": "12:00 - 1:00 pm",
        "speaker": "Prof. Hsu",
        "affiliation": "Columbia University",
        "abstract": "Tensor PCA is a model statistical inference problem introduced by Montanari and Richard in 2014 for studying method-of-moments approaches to parameter estimation in latent variable models. Unlike the matrix counterpart of the problem, Tensor PCA exhibits a computational-statistical gap in the sample-size regime where the problem is information-theoretically solvable but no computationally-efficient algorithm is known. I will describe unconditional computational lower bounds on classes of algorithms for solving Tensor PCA that shed light on limitations of commonly-used solution approaches, including gradient descent and power iteration, as well as the role of overparameterization. This talk is based on joint work with Rishabh Dudeja.",
        "bio": ""
      },
      {
        "seminar_title": "Conservative Diffusions as Entropic Flows of Steepest Descent",
        "date": "5-Nov-25",
        "location": "Room 1025 SSW, 1255 Amsterdam Avenue",
        "time": "12:00 - 1:00 pm",
        "speaker": "Prof. Ioannis Karatzas",
        "affiliation": "Columbia University",
        "abstract": "We provide a detailed, probabilistic interpretation for the variational characterization of conservative diffusions as entropic flows of steepest descent. Jordan, Kinderlehrer, and Otto showed in 1998, via a numerical scheme, that for diffusions of Langevin-Smoluchowski type the Fokker-Planck probability density flow minimizes the rate of relative entropy dissipation, as measured by the distance traveled in terms of the quadratic Wasserstein metric in the ambient space of configurations. Using a very direct perturbation analysis we obtain novel, stochastic-process versions of such features; these are valid along almost every trajectory of the motion in both the forward and, most transparently, the backward, directions of time. The original results follow then simply by \u201caggregating\u201d, i.e., by taking expectations. As a bonus we obtain a version of the HWI inequality of Otto and Villani, relating relative entropy, Fisher information, and Wasserstein distance.",
        "bio": ""
      },
      {
        "seminar_title": "Wasserstein-Cramer-Rao Theory of Unbiased Estimation",
        "date": "12-Nov-25",
        "location": "Room 1025 SSW, 1255 Amsterdam Avenue",
        "time": "12:00 - 1:00 pm",
        "speaker": "Prof. Bodhi Sen",
        "affiliation": "Columbia University",
        "abstract": "The quantity of interest in the classical Cramer-Rao theory of unbiased estimation (i.e., the Cramer-Rao lower bound, exact efficiency in exponential families, and asymptotic efficiency of maximum likelihood estimation) is the variance, which represents the instability of an estimator when its value is compared to the value for an independently-sampled data set from the same distribution. In this paper we are interested in a quantity which represents the instability of an estimator when its value is compared to the value for an infinitesimal additive perturbation of the original data set; we refer to this as the \"sensitivity\" of an estimator. The resulting theory of sensitivity is based on the Wasserstein geometry in the same way that the classical theory of variance is based on the Fisher-Rao (equivalently, Hellinger) geometry, and this insight allows us to determine a collection of results which are analogous to the classical case: a Wasserstein-Cramer-Rao lower bound for the sensitivity of any unbiased estimator, a characterization of models in which there exist unbiased estimators achieving the lower bound exactly, and a guarantee that Wasserstein projection estimators achieve the lower bound asymptotically. We use these results to treat many statistical examples, sometimes revealing new optimality properties for existing estimators and other times revealing new estimators. This is joint work with Nicolas Garcia Trillos (U Wisconsin) and Adam Jaffe (Columbia).",
        "bio": ""
      },
      {
        "seminar_title": "Variational Deep Learning via Implicit Regularization",
        "date": "19-Nov-25",
        "location": "Room 1025 SSW, 1255 Amsterdam Avenue",
        "time": "12:00 - 1:00 pm",
        "speaker": "Post-Docs - Beau Coker & Jonathan Wenger",
        "affiliation": "Columbia University",
        "abstract": "Modern deep learning models generalize remarkably well in-distribution, despite being overparameterized and trained with little to no explicit regularization. Instead, current theory credits implicit regularization imposed by the choice of architecture, hyperparameters and optimization procedure. However, deep neural networks can be surprisingly non-robust, resulting in overconfident predictions and poor out-of-distribution generalization. Bayesian deep learning addresses this via model averaging, but typically requires significant computational resources as well as carefully elicited priors to avoid overriding the benefits of implicit regularization. In this talk, we introduce a new way to regularize variational neural networks solely by relying on the implicit bias of (stochastic) gradient descent. We theoretically characterize this inductive bias in over parameterized linear models as generalized variational inference and demonstrate the importance of the choice of parametrization. Empirically, our approach demonstrates strong in- and out-of-distribution performance without additional hyperparameter tuning and with minimal computational overhead.",
        "bio": ""
      },
      {
        "seminar_title": "Breakdown point of transport-based quantiles",
        "date": "3-Dec-25",
        "location": "Room 1025 SSW, 1255 Amsterdam Avenue",
        "time": "12:00 - 1:00 pm",
        "speaker": "Prof. Alberto Gonzalez-Sanz",
        "affiliation": "",
        "abstract": "",
        "bio": ""
      },
      {
        "seminar_title": "",
        "date": "10-Dec-25",
        "location": "Room 1025 SSW, 1255 Amsterdam Avenue",
        "time": "12:00 - 1:00 pm",
        "speaker": "",
        "affiliation": "",
        "abstract": "",
        "bio": ""
      }
    ]
  },
  {
    "department": "Statistics",
    "series": "Mathematical Finance Seminar Series",
    "entries": [
      {
        "seminar_title": "The Optimal Mean-Variance Selling Problem with Finite Horizon",
        "date": "29-Jan-26",
        "location": "Room 903, 1255 Amsterdam Ave.",
        "time": "4:10 pm - 5:25 pm",
        "speaker": "Goran Peskir",
        "affiliation": "University of Manchester",
        "abstract": "Imagine an investor who owns a stock that he wishes to sell so as to maximize his return and minimize his risk upon selling. In line with the classic mean-variance analysis, we identify the return with the expectation of the stock price and the risk with the variance of the stock price.\nThe quadratic nonlinearity of the variance then moves the resulting optimal stopping problem outside the scope of the standard/linear optimal stopping theory. Consequently, the results and methods of the standard/linear optimal stopping theory are no longer applicable in this new/nonlinear setting.\nThe solution to the nonlinear problem when the horizon is infinite has been known for some time, however, the method of proof used to solve the nonlinear problem in that case is not applicable in the case when the horizon is finite. The purpose of the talk is to present a new method of proof which solves the nonlinear problem when the horizon is finite. [This is joint work with P. Johnson and J.L. Pedersen]",
        "bio": "N/A"
      },
      {
        "seminar_title": "N/A",
        "date": "5-Feb-26",
        "location": "Room 903, 1255 Amsterdam Ave.",
        "time": "4:10 pm - 5:25 pm",
        "speaker": "Jim Gatheral and Marting Larsson",
        "affiliation": "NYU",
        "abstract": "N/A",
        "bio": "N/A"
      },
      {
        "seminar_title": "N/A",
        "date": "19-Feb-26",
        "location": "Room 903, 1255 Amsterdam Ave.",
        "time": "4:10 pm - 5:25 pm",
        "speaker": "Yucheng Guo",
        "affiliation": "Princeton",
        "abstract": "N/A",
        "bio": "N/A"
      }
    ]
  }
]